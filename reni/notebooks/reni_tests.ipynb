{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing min and max values of the dataset...\n",
      "Min and max values of the dataset are (-8.033231735229492, 5.505331516265869).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Setting up training dataset<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Setting up training dataset\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Caching all <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3346</span> images.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Caching all \u001b[1;36m3346\u001b[0m images.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Warning: If you run out of memory, try reducing the number of images to sample from.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mWarning: If you run out of memory, try reducing the number of images to sample from.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ea14bcd7964a9ea53d64ae6062387b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Setting up evaluation dataset<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Setting up evaluation dataset\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Caching all <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span> images.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Caching all \u001b[1;36m21\u001b[0m images.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd99e604ee248398fb68677b927ff6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/workspace/\")\n",
    "import sys\n",
    "sys.path.append(\"/workspace/reni_neus\")\n",
    "\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import plotly.graph_objects as go\n",
    "from torch.utils.data import Dataset\n",
    "import imageio\n",
    "\n",
    "from nerfstudio.configs import base_config as cfg\n",
    "from nerfstudio.configs.method_configs import method_configs\n",
    "from nerfstudio.data.dataparsers.nerfosr_dataparser import NeRFOSR, NeRFOSRDataParserConfig\n",
    "from nerfstudio.pipelines.base_pipeline import VanillaDataManager\n",
    "from nerfstudio.field_components.field_heads import FieldHeadNames\n",
    "from nerfstudio.cameras.rays import RayBundle\n",
    "from nerfstudio.utils.colormaps import apply_depth_colormap\n",
    "from nerfstudio.field_components.encodings import SHEncoding, NeRFEncoding\n",
    "import tinycudann as tcnn\n",
    "\n",
    "from reni.reni_config import RENIField\n",
    "from reni.field_components.field_heads import RENIFieldHeadNames\n",
    "from reni.data.reni_datamanager import RENIDataManagerConfig, RENIDataManager\n",
    "\n",
    "def rotation_matrix(axis: np.ndarray, angle: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return 3D rotation matrix for rotating around the given axis by the given angle.\n",
    "    \"\"\"\n",
    "    axis = np.asarray(axis)\n",
    "    axis = axis / np.sqrt(np.dot(axis, axis))\n",
    "    a = np.cos(angle / 2.0)\n",
    "    b, c, d = -axis * np.sin(angle / 2.0)\n",
    "    aa, bb, cc, dd = a * a, b * b, c * c, d * d\n",
    "    bc, ad, ac, ab, bd, cd = b * c, a * d, a * c, a * b, b * d, c * d\n",
    "    rotation = np.array([[aa + bb - cc - dd, 2 * (bc + ad), 2 * (bd - ac)],\n",
    "                     [2 * (bc - ad), aa + cc - bb - dd, 2 * (cd + ab)],\n",
    "                     [2 * (bd + ac), 2 * (cd - ab), aa + dd - bb - cc]])\n",
    "    # convert to pytorch\n",
    "    rotation = torch.from_numpy(rotation).float()\n",
    "    return rotation\n",
    "\n",
    "# setup config\n",
    "test_mode = 'val'\n",
    "world_size = 1\n",
    "local_rank = 0\n",
    "device = 'cuda:0'\n",
    "\n",
    "reni_ckpt_path = '/workspace/outputs/unnamed/reni/2023-07-24_145239/' # model without vis\n",
    "step = 50000\n",
    "\n",
    "ckpt = torch.load(reni_ckpt_path + '/nerfstudio_models' + f'/step-{step:09d}.ckpt', map_location=device)\n",
    "reni_model_dict = {}\n",
    "for key in ckpt['pipeline'].keys():\n",
    "    if key.startswith('_model.'):\n",
    "        reni_model_dict[key[7:]] = ckpt['pipeline'][key]\n",
    "\n",
    "datamanager: RENIDataManager = RENIField.config.pipeline.datamanager.setup(\n",
    "    device=device, test_mode=test_mode, world_size=world_size, local_rank=local_rank, \n",
    ")\n",
    "datamanager.to(device)\n",
    "\n",
    "num_train_data = datamanager.num_train\n",
    "num_eval_data = datamanager.num_eval\n",
    "\n",
    "# instantiate model with config with vis\n",
    "model = RENIField.config.pipeline.model.setup(\n",
    "    scene_box=datamanager.train_dataset.scene_box,\n",
    "    num_train_data=num_train_data,\n",
    "    num_eval_data=num_eval_data,\n",
    "    metadata=datamanager.train_dataset.metadata,\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "model.load_state_dict(reni_model_dict)\n",
    "model.eval()\n",
    "\n",
    "print('Model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m i \u001b[39m=\u001b[39m i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m----> 2\u001b[0m ray_bundle, batch \u001b[39m=\u001b[39m datamanager\u001b[39m.\u001b[39;49mfixed_indices_eval_dataloader\u001b[39m.\u001b[39;49mget_data_from_image_idx(\u001b[39m0\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m H, W \u001b[39m=\u001b[39m ray_bundle\u001b[39m.\u001b[39mshape\n\u001b[1;32m      4\u001b[0m ray_bundle \u001b[39m=\u001b[39m ray_bundle\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/workspace/nerfstudio/data/utils/dataloaders.py:186\u001b[0m, in \u001b[0;36mEvalDataloader.get_data_from_image_idx\u001b[0;34m(self, image_idx)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_data_from_image_idx\u001b[39m(\u001b[39mself\u001b[39m, image_idx: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[RayBundle, Dict]:\n\u001b[1;32m    181\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns the data for a specific image index.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \n\u001b[1;32m    183\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[39m        image_idx: Camera image index\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     ray_bundle \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcameras\u001b[39m.\u001b[39;49mgenerate_rays(camera_indices\u001b[39m=\u001b[39;49mimage_idx, keep_shape\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    187\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_dataset[image_idx]\n\u001b[1;32m    188\u001b[0m     batch \u001b[39m=\u001b[39m get_dict_to_torch(batch, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice, exclude\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m/workspace/nerfstudio/cameras/cameras.py:391\u001b[0m, in \u001b[0;36mCameras.generate_rays\u001b[0;34m(self, camera_indices, coords, camera_opt_to_camera, distortion_params_delta, keep_shape, disable_distortion, aabb_box)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(camera_indices, \u001b[39mint\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[1;32m    389\u001b[0m         \u001b[39mlen\u001b[39m(cameras\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    390\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mcamera_indices must be a tensor if cameras are batched with more than 1 batch dimension\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 391\u001b[0m     camera_indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor([camera_indices], device\u001b[39m=\u001b[39;49mcameras\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    393\u001b[0m \u001b[39massert\u001b[39;00m camera_indices\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\n\u001b[1;32m    394\u001b[0m     cameras\u001b[39m.\u001b[39mshape\n\u001b[1;32m    395\u001b[0m ), \u001b[39m\"\u001b[39m\u001b[39mcamera_indices must have shape (num_rays:..., num_cameras_batch_dims)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    397\u001b[0m \u001b[39m# If keep_shape is True, then we need to make sure that the camera indices in question\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[39m# are all the same height and width and can actually be batched while maintaining the image\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[39m# shape\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "i = i + 1\n",
    "ray_bundle, batch = datamanager.fixed_indices_eval_dataloader.get_data_from_image_idx(0)\n",
    "H, W = ray_bundle.shape\n",
    "ray_bundle = ray_bundle.reshape(-1)\n",
    "\n",
    "rotation = rotation_matrix(np.array([0, 1, 0]), np.deg2rad(0))\n",
    "rotation = rotation.to(device)\n",
    "\n",
    "# field_outputs = model.field.forward(ray_bundle, rotation=rotation, latent_codes=torch.randn(1, 36, 3).to(device))\n",
    "field_outputs = model.field.forward(ray_bundle, rotation=rotation, latent_codes=torch.zeros(1, 36, 3).to(device))\n",
    "\n",
    "outputs = {\n",
    "    \"rgb\": field_outputs[RENIFieldHeadNames.RGB],\n",
    "    \"mu\": field_outputs[RENIFieldHeadNames.MU],\n",
    "    \"log_var\": field_outputs[RENIFieldHeadNames.LOG_VAR],\n",
    "}\n",
    "\n",
    "outputs['rgb'] = outputs['rgb'].reshape(H, W, 3)\n",
    "\n",
    "metrics_dict, image_dict = model.get_image_metrics_and_images(outputs, batch)\n",
    "\n",
    "plt.imshow(image_dict['img'].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
