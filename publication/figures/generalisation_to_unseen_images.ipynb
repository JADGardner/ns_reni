{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import re\n",
    "from typing import Optional\n",
    "from nerfstudio.cameras.rays import RaySamples, Frustums\n",
    "from nerfstudio.cameras.cameras import Cameras, CameraType\n",
    "\n",
    "from reni.configs.reni_config import RENIField\n",
    "from reni.pipelines.reni_pipeline import RENIPipeline\n",
    "from reni.field_components.field_heads import RENIFieldHeadNames\n",
    "from reni.data.datamanagers.reni_datamanager import RENIDataManager\n",
    "from reni.utils.utils import find_nerfstudio_project_root, rot_z, rot_y\n",
    "from reni.utils.colourspace import linear_to_sRGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup config\n",
    "test_mode = 'val'\n",
    "world_size = 1\n",
    "local_rank = 0\n",
    "device = 'cuda:0'\n",
    "\n",
    "project_root = find_nerfstudio_project_root(Path(os.getcwd()))\n",
    "# set current working directory to nerfstudio project root\n",
    "os.chdir(project_root)\n",
    "\n",
    "def load_model(load_dir: Path, load_step: Optional[int] = None):\n",
    "    ckpt_dir = load_dir / 'nerfstudio_models'\n",
    "    def clean_and_load_yaml(yaml_content):\n",
    "        # Remove !!python related tags\n",
    "        cleaned_content = re.sub(r'!!python[^\\s]*', '', yaml_content)\n",
    "        \n",
    "        # Load the cleaned content\n",
    "        return yaml.safe_load(cleaned_content)\n",
    "\n",
    "    if load_step is None:\n",
    "        load_step = sorted(int(x[x.find(\"-\") + 1 : x.find(\".\")]) for x in os.listdir(ckpt_dir))[-1]\n",
    "    \n",
    "    ckpt = torch.load(ckpt_dir / f'step-{load_step:09d}.ckpt', map_location=device)\n",
    "    reni_model_dict = {}\n",
    "    for key in ckpt['pipeline'].keys():\n",
    "        if key.startswith('_model.'):\n",
    "            reni_model_dict[key[7:]] = ckpt['pipeline'][key]\n",
    "    \n",
    "    config_path = load_dir / 'config.yml'\n",
    "    with open(config_path, 'r') as f:\n",
    "        content = f.read()\n",
    "        config = clean_and_load_yaml(content)\n",
    "    \n",
    "    reni_field_config = RENIField.config\n",
    "\n",
    "    reni_field_config.pipeline.datamanager.dataparser.convert_to_ldr = config['pipeline']['datamanager']['dataparser']['convert_to_ldr']\n",
    "    reni_field_config.pipeline.datamanager.dataparser.convert_to_log_domain = config['pipeline']['datamanager']['dataparser']['convert_to_log_domain']\n",
    "    if config['pipeline']['datamanager']['dataparser']['min_max_normalize'].__class__ == list:\n",
    "        reni_field_config.pipeline.datamanager.dataparser.min_max_normalize = tuple(config['pipeline']['datamanager']['dataparser']['min_max_normalize'])\n",
    "    else:\n",
    "        reni_field_config.pipeline.datamanager.dataparser.min_max_normalize = config['pipeline']['datamanager']['dataparser']['min_max_normalize']\n",
    "    reni_field_config.pipeline.datamanager.dataparser.augment_with_mirror = config['pipeline']['datamanager']['dataparser']['augment_with_mirror']\n",
    "    reni_field_config.pipeline.model.loss_inclusions = config['pipeline']['model']['loss_inclusions']\n",
    "    reni_field_config.pipeline.model.field.conditioning = config['pipeline']['model']['field']['conditioning']\n",
    "    reni_field_config.pipeline.model.field.invariant_function = config['pipeline']['model']['field']['invariant_function']\n",
    "    reni_field_config.pipeline.model.field.equivariance = config['pipeline']['model']['field']['equivariance']\n",
    "    reni_field_config.pipeline.model.field.axis_of_invariance = config['pipeline']['model']['field']['axis_of_invariance']\n",
    "    reni_field_config.pipeline.model.field.positional_encoding = config['pipeline']['model']['field']['positional_encoding']\n",
    "    reni_field_config.pipeline.model.field.encoded_input = config['pipeline']['model']['field']['encoded_input']\n",
    "    reni_field_config.pipeline.model.field.latent_dim = config['pipeline']['model']['field']['latent_dim']\n",
    "    reni_field_config.pipeline.model.field.hidden_features = config['pipeline']['model']['field']['hidden_features']\n",
    "    reni_field_config.pipeline.model.field.hidden_layers = config['pipeline']['model']['field']['hidden_layers']\n",
    "    reni_field_config.pipeline.model.field.mapping_layers = config['pipeline']['model']['field']['mapping_layers']\n",
    "    reni_field_config.pipeline.model.field.mapping_features = config['pipeline']['model']['field']['mapping_features']\n",
    "    reni_field_config.pipeline.model.field.num_attention_heads = config['pipeline']['model']['field']['num_attention_heads']\n",
    "    reni_field_config.pipeline.model.field.num_attention_layers = config['pipeline']['model']['field']['num_attention_layers']\n",
    "    reni_field_config.pipeline.model.field.output_activation = config['pipeline']['model']['field']['output_activation']\n",
    "    reni_field_config.pipeline.model.field.last_layer_linear = config['pipeline']['model']['field']['last_layer_linear']\n",
    "    reni_field_config.pipeline.model.field.trainable_scale = config['pipeline']['model']['field']['trainable_scale']\n",
    "    reni_field_config.pipeline.model.field.old_implementation = config['pipeline']['model']['field']['old_implementation']\n",
    "    reni_field_config.pipeline.model.loss_inclusions = config['pipeline']['model']['loss_inclusions']\n",
    "\n",
    "    pipeline: RENIPipeline = reni_field_config.pipeline.setup(\n",
    "      device=device,\n",
    "      test_mode=test_mode,\n",
    "      world_size=world_size,\n",
    "      local_rank=local_rank,\n",
    "      grad_scaler=None,\n",
    "    )\n",
    "\n",
    "    datamanager = pipeline.datamanager\n",
    "\n",
    "    model = pipeline.model\n",
    "\n",
    "    model.to(device)\n",
    "    print(model.field.train_mu.shape)\n",
    "    model.load_state_dict(reni_model_dict)\n",
    "    model.eval()\n",
    "\n",
    "    return pipeline, datamanager, model\n",
    "\n",
    "def generate_images_from_models(image_indices, model_paths):\n",
    "    all_model_outputs = {}\n",
    "    \n",
    "    for model_path in model_paths:\n",
    "        model_name = model_path.split(\"/\")[-1]\n",
    "        pipeline, datamanager, model = load_model(Path(model_path))\n",
    "        \n",
    "        model_outputs = {}\n",
    "        \n",
    "        for idx in image_indices:\n",
    "            # Your code to produce an image would go here.\n",
    "            model.eval()\n",
    "            _, ray_bundle, batch = datamanager.next_eval_image(idx)\n",
    "            H, W = model.metadata[\"image_height\"], model.metadata[\"image_width\"]\n",
    "\n",
    "            # High res image:\n",
    "            H = 128\n",
    "            W = H * 2\n",
    "            cx = torch.tensor(W // 2, dtype=torch.float32).repeat(1)\n",
    "            cy = torch.tensor(H // 2, dtype=torch.float32).repeat(1)\n",
    "            fx = torch.tensor(H, dtype=torch.float32).repeat(1)\n",
    "            fy = torch.tensor(H, dtype=torch.float32).repeat(1)\n",
    "\n",
    "            c2w = torch.tensor([[[1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0]]], dtype=torch.float32).repeat(1, 1, 1)\n",
    "\n",
    "            cameras = Cameras(fx=fx, fy=fy, cx=cx, cy=cy, camera_to_worlds=c2w, camera_type=CameraType.EQUIRECTANGULAR)\n",
    "\n",
    "            ray_bundle = cameras.generate_rays(0).flatten().to(device)\n",
    "            ray_bundle.camera_indices = torch.ones_like(ray_bundle.camera_indices) * idx\n",
    "\n",
    "            batch['image'] = batch['image'].to(device)\n",
    "\n",
    "            if model.field.old_implementation:\n",
    "                get_rotation = rot_y\n",
    "            else:\n",
    "                get_rotation = rot_z\n",
    "\n",
    "            rotation = get_rotation(torch.tensor(np.deg2rad(0.0)).float())\n",
    "            rotation = rotation.to(device)\n",
    "\n",
    "            outputs = model.get_outputs_for_camera_ray_bundle(ray_bundle)\n",
    "            outputs['rgb'] = outputs['rgb'].reshape(H, W, 3)\n",
    "            pred_img = model.field.unnormalise(outputs['rgb'])\n",
    "            pred_img = linear_to_sRGB(pred_img, use_quantile=True)\n",
    "            model_outputs[idx] = pred_img\n",
    "            \n",
    "        all_model_outputs[model_name] = model_outputs\n",
    "    \n",
    "    return all_model_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Setting up training dataset<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Setting up training dataset\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Caching all <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3346</span> images.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Caching all \u001b[1;36m3346\u001b[0m images.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Warning: If you run out of memory, try reducing the number of images to sample from.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mWarning: If you run out of memory, try reducing the number of images to sample from.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5583ab062c854664b673a96f77c954fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Setting up evaluation dataset<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Setting up evaluation dataset\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Caching all <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span> images.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Caching all \u001b[1;36m21\u001b[0m images.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e7890feedb454b87a57245a972e198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3346, 9, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Setting up training dataset<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Setting up training dataset\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Caching all <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3346</span> images.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Caching all \u001b[1;36m3346\u001b[0m images.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Warning: If you run out of memory, try reducing the number of images to sample from.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mWarning: If you run out of memory, try reducing the number of images to sample from.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79bc29e0fcdb42c7952e33b4abb84617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Setting up evaluation dataset<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Setting up evaluation dataset\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Caching all <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span> images.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Caching all \u001b[1;36m21\u001b[0m images.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cba2bd2e3354a04b356e773814cd46e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3346, 100, 3])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "image_indices = [1, 2, 3]\n",
    "model_paths = [\n",
    "    '/workspace/outputs/reni/reni_plus_plus_models/latent_dim_9',\n",
    "    '/workspace/outputs/reni/reni_plus_plus_models/latent_dim_100'\n",
    "]\n",
    "\n",
    "output_images = generate_images_from_models(image_indices, model_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: tensor([[[0.2142, 0.3746, 0.5365],\n",
       "          [0.2142, 0.3746, 0.5364],\n",
       "          [0.2142, 0.3746, 0.5364],\n",
       "          ...,\n",
       "          [0.2143, 0.3748, 0.5366],\n",
       "          [0.2143, 0.3747, 0.5365],\n",
       "          [0.2143, 0.3747, 0.5365]],\n",
       " \n",
       "         [[0.2207, 0.3818, 0.5429],\n",
       "          [0.2206, 0.3817, 0.5427],\n",
       "          [0.2205, 0.3815, 0.5424],\n",
       "          ...,\n",
       "          [0.2210, 0.3823, 0.5434],\n",
       "          [0.2209, 0.3822, 0.5432],\n",
       "          [0.2208, 0.3820, 0.5430]],\n",
       " \n",
       "         [[0.2207, 0.3826, 0.5432],\n",
       "          [0.2207, 0.3825, 0.5432],\n",
       "          [0.2207, 0.3824, 0.5431],\n",
       "          ...,\n",
       "          [0.2206, 0.3828, 0.5433],\n",
       "          [0.2206, 0.3827, 0.5433],\n",
       "          [0.2207, 0.3826, 0.5432]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.6215, 0.6048, 0.5447],\n",
       "          [0.6212, 0.6045, 0.5444],\n",
       "          [0.6209, 0.6041, 0.5441],\n",
       "          ...,\n",
       "          [0.6224, 0.6057, 0.5454],\n",
       "          [0.6221, 0.6054, 0.5452],\n",
       "          [0.6218, 0.6051, 0.5449]],\n",
       " \n",
       "         [[0.6154, 0.5982, 0.5379],\n",
       "          [0.6152, 0.5980, 0.5377],\n",
       "          [0.6150, 0.5978, 0.5375],\n",
       "          ...,\n",
       "          [0.6159, 0.5988, 0.5383],\n",
       "          [0.6157, 0.5986, 0.5382],\n",
       "          [0.6156, 0.5984, 0.5380]],\n",
       " \n",
       "         [[0.6060, 0.5884, 0.5281],\n",
       "          [0.6059, 0.5884, 0.5281],\n",
       "          [0.6059, 0.5883, 0.5280],\n",
       "          ...,\n",
       "          [0.6062, 0.5886, 0.5283],\n",
       "          [0.6061, 0.5885, 0.5283],\n",
       "          [0.6061, 0.5885, 0.5282]]], device='cuda:0'),\n",
       " 2: tensor([[[0.3053, 0.5120, 0.7403],\n",
       "          [0.3054, 0.5121, 0.7404],\n",
       "          [0.3056, 0.5123, 0.7406],\n",
       "          ...,\n",
       "          [0.3048, 0.5114, 0.7398],\n",
       "          [0.3050, 0.5116, 0.7400],\n",
       "          [0.3051, 0.5118, 0.7402]],\n",
       " \n",
       "         [[0.3104, 0.5165, 0.7471],\n",
       "          [0.3109, 0.5170, 0.7475],\n",
       "          [0.3113, 0.5174, 0.7478],\n",
       "          ...,\n",
       "          [0.3090, 0.5150, 0.7459],\n",
       "          [0.3095, 0.5155, 0.7463],\n",
       "          [0.3100, 0.5160, 0.7467]],\n",
       " \n",
       "         [[0.3101, 0.5159, 0.7485],\n",
       "          [0.3109, 0.5167, 0.7492],\n",
       "          [0.3117, 0.5175, 0.7499],\n",
       "          ...,\n",
       "          [0.3077, 0.5135, 0.7467],\n",
       "          [0.3085, 0.5143, 0.7473],\n",
       "          [0.3093, 0.5151, 0.7479]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.4338, 0.4283, 0.2144],\n",
       "          [0.4338, 0.4283, 0.2145],\n",
       "          [0.4339, 0.4284, 0.2146],\n",
       "          ...,\n",
       "          [0.4335, 0.4281, 0.2140],\n",
       "          [0.4336, 0.4282, 0.2142],\n",
       "          [0.4337, 0.4282, 0.2143]],\n",
       " \n",
       "         [[0.4390, 0.4329, 0.2192],\n",
       "          [0.4390, 0.4329, 0.2193],\n",
       "          [0.4390, 0.4329, 0.2193],\n",
       "          ...,\n",
       "          [0.4389, 0.4329, 0.2190],\n",
       "          [0.4390, 0.4329, 0.2191],\n",
       "          [0.4390, 0.4329, 0.2191]],\n",
       " \n",
       "         [[0.4415, 0.4348, 0.2229],\n",
       "          [0.4415, 0.4348, 0.2229],\n",
       "          [0.4415, 0.4348, 0.2229],\n",
       "          ...,\n",
       "          [0.4414, 0.4347, 0.2228],\n",
       "          [0.4414, 0.4348, 0.2228],\n",
       "          [0.4414, 0.4348, 0.2229]]], device='cuda:0'),\n",
       " 3: tensor([[[0.4529, 0.4915, 0.5069],\n",
       "          [0.4529, 0.4915, 0.5069],\n",
       "          [0.4530, 0.4915, 0.5068],\n",
       "          ...,\n",
       "          [0.4528, 0.4914, 0.5070],\n",
       "          [0.4528, 0.4915, 0.5070],\n",
       "          [0.4529, 0.4915, 0.5069]],\n",
       " \n",
       "         [[0.4546, 0.4929, 0.5082],\n",
       "          [0.4547, 0.4929, 0.5081],\n",
       "          [0.4548, 0.4930, 0.5080],\n",
       "          ...,\n",
       "          [0.4543, 0.4929, 0.5085],\n",
       "          [0.4544, 0.4929, 0.5084],\n",
       "          [0.4545, 0.4929, 0.5083]],\n",
       " \n",
       "         [[0.4564, 0.4947, 0.5087],\n",
       "          [0.4570, 0.4952, 0.5092],\n",
       "          [0.4576, 0.4958, 0.5098],\n",
       "          ...,\n",
       "          [0.4549, 0.4934, 0.5076],\n",
       "          [0.4554, 0.4938, 0.5079],\n",
       "          [0.4559, 0.4942, 0.5083]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.1884, 0.1628, 0.0969],\n",
       "          [0.1884, 0.1628, 0.0969],\n",
       "          [0.1883, 0.1627, 0.0968],\n",
       "          ...,\n",
       "          [0.1885, 0.1628, 0.0971],\n",
       "          [0.1885, 0.1628, 0.0970],\n",
       "          [0.1885, 0.1628, 0.0970]],\n",
       " \n",
       "         [[0.1877, 0.1626, 0.0963],\n",
       "          [0.1877, 0.1626, 0.0963],\n",
       "          [0.1877, 0.1626, 0.0963],\n",
       "          ...,\n",
       "          [0.1877, 0.1626, 0.0963],\n",
       "          [0.1877, 0.1626, 0.0963],\n",
       "          [0.1877, 0.1626, 0.0963]],\n",
       " \n",
       "         [[0.1872, 0.1626, 0.0960],\n",
       "          [0.1872, 0.1626, 0.0960],\n",
       "          [0.1872, 0.1626, 0.0960],\n",
       "          ...,\n",
       "          [0.1872, 0.1626, 0.0960],\n",
       "          [0.1872, 0.1626, 0.0960],\n",
       "          [0.1872, 0.1626, 0.0960]]], device='cuda:0')}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_images['latent_dim_100']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
