{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Setting up training dataset<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Setting up training dataset\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Caching all <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3346</span> images.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Caching all \u001b[1;36m3346\u001b[0m images.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Warning: If you run out of memory, try reducing the number of images to sample from.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mWarning: If you run out of memory, try reducing the number of images to sample from.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ff6058b2b54c1a87d283c9df498638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Setting up evaluation dataset<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Setting up evaluation dataset\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Caching all <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span> images.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Caching all \u001b[1;36m21\u001b[0m images.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98611e584385430dbe764b6e2fb14efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import re\n",
    "from typing import Optional\n",
    "from tqdm import tqdm\n",
    "from nerfstudio.cameras.rays import RaySamples, Frustums\n",
    "from nerfstudio.cameras.cameras import Cameras, CameraType\n",
    "from nerfstudio.utils import colormaps, misc\n",
    "\n",
    "from reni.configs.reni_config import RENIField\n",
    "from reni.configs.sh_sg_envmap_configs import SHField, SGField\n",
    "from reni.pipelines.reni_pipeline import RENIPipeline\n",
    "from reni.field_components.field_heads import RENIFieldHeadNames\n",
    "from reni.data.datamanagers.reni_datamanager import RENIDataManager\n",
    "from reni.utils.utils import find_nerfstudio_project_root, rot_z, rot_y\n",
    "from reni.utils.colourspace import linear_to_sRGB\n",
    "\n",
    "# setup config\n",
    "world_size = 1\n",
    "local_rank = 0\n",
    "device = 'cuda:0'\n",
    "\n",
    "project_root = find_nerfstudio_project_root(Path(os.getcwd()))\n",
    "# set current working directory to nerfstudio project root\n",
    "os.chdir(project_root)\n",
    "\n",
    "def load_model(load_dir: Path, load_step: Optional[int] = None):\n",
    "    ckpt_dir = load_dir / 'nerfstudio_models'\n",
    "    def clean_and_load_yaml(yaml_content):\n",
    "        # Remove !!python related tags\n",
    "        cleaned_content = re.sub(r'!!python[^\\s]*', '', yaml_content)\n",
    "        \n",
    "        # Load the cleaned content\n",
    "        return yaml.safe_load(cleaned_content)\n",
    "\n",
    "    if load_step is None:\n",
    "        load_step = sorted(int(x[x.find(\"-\") + 1 : x.find(\".\")]) for x in os.listdir(ckpt_dir))[-1]\n",
    "    \n",
    "    ckpt = torch.load(ckpt_dir / f'step-{load_step:09d}.ckpt', map_location=device)\n",
    "    reni_model_dict = {}\n",
    "    for key in ckpt['pipeline'].keys():\n",
    "        if key.startswith('_model.'):\n",
    "            reni_model_dict[key[7:]] = ckpt['pipeline'][key]\n",
    "    \n",
    "    config_path = load_dir / 'config.yml'\n",
    "    with open(config_path, 'r') as f:\n",
    "        content = f.read()\n",
    "        config = clean_and_load_yaml(content)\n",
    "    \n",
    "    if 'latent_dim' in config['pipeline']['model']['field'].keys():\n",
    "        \n",
    "        model_config = RENIField.config\n",
    "        model_config.pipeline.datamanager.dataparser.convert_to_ldr = config['pipeline']['datamanager']['dataparser']['convert_to_ldr']\n",
    "        model_config.pipeline.datamanager.dataparser.convert_to_log_domain = config['pipeline']['datamanager']['dataparser']['convert_to_log_domain']\n",
    "        if config['pipeline']['datamanager']['dataparser']['eval_mask_path'] is not None:\n",
    "            eval_mask_path = Path(os.path.join(*config['pipeline']['datamanager']['dataparser']['eval_mask_path']))\n",
    "            model_config.pipeline.datamanager.dataparser.eval_mask_path = eval_mask_path\n",
    "        else:\n",
    "            model_config.pipeline.datamanager.dataparser.eval_mask_path = None\n",
    "        if config['pipeline']['datamanager']['dataparser']['min_max_normalize'].__class__ == list:\n",
    "            model_config.pipeline.datamanager.dataparser.min_max_normalize = tuple(config['pipeline']['datamanager']['dataparser']['min_max_normalize'])\n",
    "        else:\n",
    "            model_config.pipeline.datamanager.dataparser.min_max_normalize = config['pipeline']['datamanager']['dataparser']['min_max_normalize']\n",
    "        model_config.pipeline.datamanager.dataparser.augment_with_mirror = config['pipeline']['datamanager']['dataparser']['augment_with_mirror']\n",
    "        model_config.pipeline.model.loss_inclusions = config['pipeline']['model']['loss_inclusions']\n",
    "        model_config.pipeline.model.field.conditioning = config['pipeline']['model']['field']['conditioning']\n",
    "        model_config.pipeline.model.field.invariant_function = config['pipeline']['model']['field']['invariant_function']\n",
    "        model_config.pipeline.model.field.equivariance = config['pipeline']['model']['field']['equivariance']\n",
    "        model_config.pipeline.model.field.axis_of_invariance = config['pipeline']['model']['field']['axis_of_invariance']\n",
    "        model_config.pipeline.model.field.positional_encoding = config['pipeline']['model']['field']['positional_encoding']\n",
    "        model_config.pipeline.model.field.encoded_input = config['pipeline']['model']['field']['encoded_input']\n",
    "        model_config.pipeline.model.field.latent_dim = config['pipeline']['model']['field']['latent_dim']\n",
    "        model_config.pipeline.model.field.hidden_features = config['pipeline']['model']['field']['hidden_features']\n",
    "        model_config.pipeline.model.field.hidden_layers = config['pipeline']['model']['field']['hidden_layers']\n",
    "        model_config.pipeline.model.field.mapping_layers = config['pipeline']['model']['field']['mapping_layers']\n",
    "        model_config.pipeline.model.field.mapping_features = config['pipeline']['model']['field']['mapping_features']\n",
    "        model_config.pipeline.model.field.num_attention_heads = config['pipeline']['model']['field']['num_attention_heads']\n",
    "        model_config.pipeline.model.field.num_attention_layers = config['pipeline']['model']['field']['num_attention_layers']\n",
    "        model_config.pipeline.model.field.output_activation = config['pipeline']['model']['field']['output_activation']\n",
    "        model_config.pipeline.model.field.last_layer_linear = config['pipeline']['model']['field']['last_layer_linear']\n",
    "        model_config.pipeline.model.field.trainable_scale = config['pipeline']['model']['field']['trainable_scale']\n",
    "        model_config.pipeline.model.field.old_implementation = config['pipeline']['model']['field']['old_implementation']\n",
    "        model_config.pipeline.model.loss_inclusions = config['pipeline']['model']['loss_inclusions']\n",
    "    elif 'spherical_harmonic_order' in config['pipeline']['model']['field'].keys():\n",
    "        model_config = SHField.config\n",
    "        model_config.pipeline.model.field.spherical_harmonic_order = config['pipeline']['model']['field']['spherical_harmonic_order']\n",
    "    elif 'row_col_gaussian_dims' in config['pipeline']['model']['field'].keys():\n",
    "        model_config = SGField.config\n",
    "        model_config.pipeline.model.field.row_col_gaussian_dims = config['pipeline']['model']['field']['row_col_gaussian_dims']\n",
    "\n",
    "    model_config.pipeline.test_mode = config['pipeline']['test_mode']\n",
    "    test_mode = model_config.pipeline.test_mode\n",
    "\n",
    "    pipeline: RENIPipeline = model_config.pipeline.setup(\n",
    "      device=device,\n",
    "      test_mode=test_mode,\n",
    "      world_size=world_size,\n",
    "      local_rank=local_rank,\n",
    "      grad_scaler=None,\n",
    "    )\n",
    "\n",
    "    datamanager = pipeline.datamanager\n",
    "\n",
    "    model = pipeline.model\n",
    "\n",
    "    model.to(device)\n",
    "    model.load_state_dict(reni_model_dict)\n",
    "    model.eval()\n",
    "\n",
    "    return pipeline, datamanager, model\n",
    "\n",
    "model_path = Path('/workspace/neusky/ns_reni/models/reni_plus_plus_models/latent_dim_100')\n",
    "pipeline, datamanager, model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from PIL import Image\n",
    "import io\n",
    "import itertools\n",
    "\n",
    "def get_3d_vector_plot(latent_code, vectors_to_show, seed=42):\n",
    "    \"\"\"\n",
    "    Plots a subset of 3D vectors from the latent code using Matplotlib and returns the plot as an image array.\n",
    "    The quivers are colored with a repeatable set of colors.\n",
    "\n",
    "    :param latent_code: torch.tensor -> [N, 3]\n",
    "    :param vectors_to_show: int\n",
    "    \"\"\"\n",
    "\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Define a list of colors\n",
    "    colors = ['red', 'green', 'blue', 'purple', 'orange', 'cyan']\n",
    "    color_cycle = itertools.cycle(colors)\n",
    "\n",
    "    fixed_arrowhead_size = 0.2  # Adjust this value as needed\n",
    "\n",
    "    # Ensure the latent code is a 2D tensor with shape [N, 3]\n",
    "    if len(latent_code.shape) != 2 or latent_code.shape[1] != 3:\n",
    "        raise ValueError(\"latent_code must be a 2D tensor with shape [N, 3]\")\n",
    "\n",
    "    # Ensure vectors_to_show is not greater than the number of vectors in latent_code\n",
    "    vectors_to_show = min(vectors_to_show, latent_code.shape[0])\n",
    "\n",
    "    # Select a random subset of vectors\n",
    "    indices = random.sample(range(latent_code.shape[0]), vectors_to_show)\n",
    "    vectors = latent_code[indices]\n",
    "\n",
    "    # Initialize 3D plot\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Add vectors to the plot\n",
    "    for vector in vectors:\n",
    "        vector = vector.cpu().numpy()  # Convert tensor to numpy array\n",
    "        length = np.linalg.norm(vector)  # Calculate the length of the vector\n",
    "\n",
    "        if length > 2:\n",
    "            # Normalise the vector to length 2\n",
    "            vector = (vector / length) * 2\n",
    "            length = 2\n",
    "\n",
    "        # Adjust arrow_length_ratio to maintain consistent arrowhead size\n",
    "        arrow_length_ratio = fixed_arrowhead_size / length\n",
    "\n",
    "        # Plot the vector with a color from the color cycle\n",
    "        color = next(color_cycle)\n",
    "        ax.quiver(0, 0, 0, vector[0], vector[1], vector[2], color=color, length=length, \n",
    "                  normalize=True, arrow_length_ratio=arrow_length_ratio)\n",
    "\n",
    "    # Setting the ticks on each axis\n",
    "    ax.set_xticks([-2, 0, 2])\n",
    "    ax.set_yticks([-2, 0, 2])\n",
    "    ax.set_zticks([-2, 0, 2])\n",
    "\n",
    "    # remove grid\n",
    "    ax.grid(False)\n",
    "\n",
    "    # Save the plot to a buffer\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png', bbox_inches='tight')\n",
    "    buf.seek(0)\n",
    "    img = Image.open(buf)\n",
    "    img_array = np.array(img)\n",
    "    buf.close()\n",
    "\n",
    "    plt.close(fig)  # Close the figure to free up memory\n",
    "\n",
    "    img_array = img_array[30:395, 20:395, :3]\n",
    "\n",
    "    return img_array\n",
    "\n",
    "def generate_rotation_animation(image_idx, frames, model, datamanager, filename='animation.m4v', fps=24, include_field_diagram=False):\n",
    "    model_outputs = {}\n",
    "        \n",
    "    for i in tqdm(range(frames)):\n",
    "        rotation_angle = i * 360 / frames\n",
    "        # Your code to produce an image would go here.\n",
    "        model.eval()\n",
    "        _, ray_bundle, batch = datamanager.next_eval_image(image_idx)\n",
    "        H, W = model.metadata[\"image_height\"], model.metadata[\"image_width\"]\n",
    "\n",
    "        # High res image:\n",
    "        H = 256\n",
    "        W = H * 2\n",
    "        cx = torch.tensor(W // 2, dtype=torch.float32).repeat(1)\n",
    "        cy = torch.tensor(H // 2, dtype=torch.float32).repeat(1)\n",
    "        fx = torch.tensor(H, dtype=torch.float32).repeat(1)\n",
    "        fy = torch.tensor(H, dtype=torch.float32).repeat(1)\n",
    "\n",
    "        c2w = torch.tensor([[[1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0]]], dtype=torch.float32).repeat(1, 1, 1)\n",
    "\n",
    "        cameras = Cameras(fx=fx, fy=fy, cx=cx, cy=cy, camera_to_worlds=c2w, camera_type=CameraType.EQUIRECTANGULAR)\n",
    "\n",
    "        ray_bundle = cameras.generate_rays(0).flatten().to(device)\n",
    "        ray_bundle.camera_indices = torch.ones_like(ray_bundle.camera_indices) * image_idx\n",
    "\n",
    "        batch['image'] = batch['image'].to(device)\n",
    "\n",
    "        # check if the model has attribute old_implementation\n",
    "        if hasattr(model.field, 'old_implementation'):\n",
    "            if model.field.old_implementation:\n",
    "                get_rotation = rot_y\n",
    "            else:\n",
    "                get_rotation = rot_z\n",
    "        else:\n",
    "            get_rotation = rot_z\n",
    "\n",
    "        rotation = get_rotation(torch.tensor(np.deg2rad(rotation_angle)).float())\n",
    "        rotation = rotation.to(device)\n",
    "\n",
    "        outputs = model.get_outputs_for_camera_ray_bundle(ray_bundle, rotation)\n",
    "\n",
    "        pred_img = model.field.unnormalise(outputs['rgb'])\n",
    "\n",
    "        pred_img = pred_img.view(H, W, 3)\n",
    "\n",
    "        pred_img = linear_to_sRGB(pred_img, use_quantile=True) # [H, W, 3]\n",
    "\n",
    "        latent_code, _, _ = model.field.sample_latent(image_idx)\n",
    "\n",
    "        rotation = get_rotation(torch.tensor(np.deg2rad(-rotation_angle)).float())\n",
    "        rotation = rotation.to(device)\n",
    "\n",
    "        latent_code = torch.matmul(latent_code, rotation)\n",
    "\n",
    "        plot = get_3d_vector_plot(latent_code.cpu().detach(), 50) # H, W, 3\n",
    "\n",
    "        # Convert numpy array to PIL Image\n",
    "        img = Image.fromarray(plot)\n",
    "\n",
    "        # Calculate the new width to maintain aspect ratio\n",
    "        original_width, original_height = img.size\n",
    "        aspect_ratio = original_width / original_height\n",
    "        new_width = int(H * aspect_ratio)\n",
    "\n",
    "        # Resize the image\n",
    "        resized_img = img.resize((new_width, H), Image.BICUBIC) # [H, W, 3]\n",
    "\n",
    "        # Convert back to numpy array\n",
    "        plot = np.array(resized_img, dtype=np.float32) / 255.0\n",
    "\n",
    "        if include_field_diagram:\n",
    "            diagram_path = '/workspace/neusky/ns_reni/publication/figures/neural_field.png'\n",
    "            diagram = Image.open(diagram_path)\n",
    "            # Calculate the new width to maintain aspect ratio\n",
    "            original_width, original_height = diagram.size\n",
    "            aspect_ratio = original_width / original_height\n",
    "            new_width = int(H * aspect_ratio)\n",
    "\n",
    "            # Resize the image\n",
    "            resized_diagram = diagram.resize((new_width, H), Image.BICUBIC)\n",
    "\n",
    "            # Convert back to numpy array\n",
    "            diagram = np.array(resized_diagram, dtype=np.float32) / 255.0\n",
    "\n",
    "            plot = np.concatenate((plot, diagram), axis=1)\n",
    "\n",
    "        # attach plot to left hand side of image\n",
    "        plot = np.concatenate((plot, pred_img.cpu().detach().numpy()), axis=1)\n",
    "\n",
    "        model_outputs[i] = {'plot': plot}\n",
    "\n",
    "    height, width, _ = model_outputs[0]['plot'].shape\n",
    "    size = (width, height)\n",
    "\n",
    "    # Create a VideoWriter object\n",
    "    out = cv2.VideoWriter(filename, cv2.VideoWriter_fourcc(*'mp4v'), fps, size)\n",
    "\n",
    "    for i in range(len(model_outputs)):\n",
    "        # Convert the plot to the correct color format\n",
    "        img = cv2.cvtColor(model_outputs[i]['plot'], cv2.COLOR_RGB2BGR)\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "        out.write(img)\n",
    "\n",
    "    # Release the VideoWriter\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:29<00:00,  3.23it/s]\n"
     ]
    }
   ],
   "source": [
    "model.field.config.view_train_latents = True\n",
    "path = '/workspace/neusky/ns_reni/publication/figures/reni_plus_plus_teaser.mp4'\n",
    "generate_rotation_animation(96, 96, model, datamanager, filename=path, fps=24, include_field_diagram=True) # 96, 136, 219, 252\n",
    "model.field.config.view_train_latents = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
